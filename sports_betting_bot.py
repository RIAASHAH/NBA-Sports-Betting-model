# -*- coding: utf-8 -*-
"""sports betting bot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HTavLJmf9LKfIv3M5rDxPzhack2CXcms
"""

import requests
import pandas as pd
from datetime import datetime, timedelta

# API details
base_url = "https://api.sportsdata.io/v3/nba/scores/json/GamesByDate/"
api_key = "32c7cba554574bed96f02a3021980e91"  # Replace with your API key
headers = {"Ocp-Apim-Subscription-Key": api_key}

# Function to fetch data for a given date
def fetch_data_for_date(date):
    url = f"{base_url}{date}?key={api_key}"
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        return None

# Function to save data to CSV
def save_to_csv(data, filename):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

# Getting the last 30 days data
current_date = datetime.now()
all_data = []
for i in range(30):
    date_to_fetch = (current_date - timedelta(days=i)).strftime("%Y-%m-%d")
    daily_data = fetch_data_for_date(date_to_fetch)
    if daily_data:
        all_data.extend(daily_data)

# Saving to CSV
filename = "NBA_Games_Last_30_Days.csv"
save_to_csv(all_data, filename)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
# Load your dataset
df = pd.read_csv('NBA_Games_Last_30_Days.csv')


# Example preprocessing
df['HomeWin'] = (df['HomeTeamScore'] > df['AwayTeamScore']).astype(int)
features = ['HomeTeam', 'AwayTeam', 'HomeTeamScore', 'AwayTeamScore'] # Correct this as per your CSV columns
df = df[features + ['HomeWin']]

# Convert team names to numerical codes and save the mappings
df['HomeTeam'] = df['HomeTeam'].astype('category')
df['AwayTeam'] = df['AwayTeam'].astype('category')
home_team_mapping = dict(enumerate(df['HomeTeam'].cat.categories))
away_team_mapping = dict(enumerate(df['AwayTeam'].cat.categories))
df['HomeTeam'] = df['HomeTeam'].cat.codes
df['AwayTeam'] = df['AwayTeam'].cat.codes

# Define features and target
X = df.drop('HomeWin', axis=1)
y = df['HomeWin']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

imputer = SimpleImputer(strategy='mean')

# Train on the training features
imputer.fit(X_train)

# Transform both training data and testing data
X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)
# Initialize and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict and evaluate
predictions = model.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, predictions):.2f}')

# Function to predict the outcome of a match
def predict_match(home_team, away_team):
    home_code = {v: k for k, v in home_team_mapping.items()}.get(home_team)
    away_code = {v: k for k, v in away_team_mapping.items()}.get(away_team)

    if home_code is None or away_code is None:
        return "One or both team names are not recognized."

    # Predict using the model
    outcome = model.predict([[home_code, away_code, 0, 0]]) # The scores are set to 0 since they're not used in prediction
    return home_team if outcome[0] == 1 else away_team

# Example prediction
print(f'Predicted Winner: {predict_match("ATL", "POR")}')

import requests
import pandas as pd
from datetime import datetime, timedelta

# Function to fetch data for a given date
def fetch_data_for_date(date, api_key):
    url = f"https://api.sportsdata.io/v3/nba/stats/json/PlayerGameStatsByDate/{date}?key={api_key}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()
    else:
        return None

# Set your API key
api_key = "32c7cba554574bed96f02a3021980e91"

# Fetch data for the last 30 days
all_player_stats = []
for offset in range(30):
    date = (datetime.now() - timedelta(days=offset)).strftime("%Y-%m-%d")
    daily_stats = fetch_data_for_date(date, api_key)
    all_player_stats.extend(daily_stats)

# Convert the data to a DataFrame
player_stats_df = pd.DataFrame(all_player_stats)

# Save the player stats to a CSV file
player_stats_filename = 'player_stats_last_30_days.csv'
player_stats_df.to_csv(player_stats_filename, index=False)
print(f"Player stats saved to {player_stats_filename}")

import pandas as pd

# Load the datasets
player_stats_df = pd.read_csv('player_stats_last_30_days.csv')
games_df = pd.read_csv('NBA_Games_Last_30_Days.csv')

# Merge the datasets on GameID
# This will repeat the game stats for each player in the same game
merged_df = pd.merge(player_stats_df, games_df, on='GameID', how='left')

# Save the merged data to a new CSV file
merged_df.to_csv('Merged_Player_and_Game_Stats.csv', index=False)

from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the merged dataset
df = pd.read_csv('Merged_Player_and_Game_Stats.csv')

# Define non-feature columns
non_feature_columns = ['GameID', 'Day', 'DateTime', 'DateTimeUTC','HomeWin']

# Dynamically create a list of features by excluding non-feature columns
features = df.columns.difference(non_feature_columns)

# Preprocess the data
df['HomeWin'] = (df['HomeTeamScore'] > df['AwayTeamScore']).astype(int)

# Include only the selected features and the target variable in the DataFrame
df = df[list(features) + ['HomeWin']]

# Identify categorical columns for encoding
categorical_cols = df.select_dtypes( include=['object', 'category']).columns

# Define numerical columns (excluding categorical and non-feature columns)
numerical_cols = df.columns.difference(categorical_cols.tolist() + non_feature_columns)

# Create preprocessors for categorical and numerical data
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', SimpleImputer(strategy='mean'), numerical_cols)
    ]
)

# Create a pipeline with preprocessing and model
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Define features and target
X = df[features]
y = df['HomeWin']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Uncomment when 'X' and 'y' are defined

# Train the logistic regression model
model.fit(X_train, y_train) # Uncomment when 'X_train' and 'y_train' are defined

# Predict and evaluate
predictions = model.predict(X_test) # Uncomment when 'X_test' is defined
print(f'Accuracy: {accuracy_score(y_test, predictions):.2f}')

import numpy as np
import pandas as pd

def predict_match(home_team, away_team, model, preprocessor, team_stats):
    # Create a DataFrame with the same structure as the training data
    data = pd.DataFrame(columns=team_stats.columns)

    # Assign values for the teams and set other columns to a default value (like 0 or mean)
    data.loc[0, 'HomeTeam'] = home_team
    data.loc[0, 'AwayTeam'] = away_team
    for col in data.columns:
        if col not in ['HomeTeam', 'AwayTeam']:
            data.loc[0, col] = 0  # Or use another default/filler value

    # Apply the same preprocessing that was done to the training data
    processed_data = preprocessor.transform(data)

    # Predict using the model
    prediction = model.predict(processed_data)

    return home_team if prediction[0] == 1 else away_team

# Example usage
predicted_winner = predict_match("HOU", "POR", model['classifier'], model['preprocessor'], df)
print(f'Predicted Winner: {predicted_winner}')

pip install praw

!pip install textblob
from textblob import TextBlob

import praw
import pandas as pd
import logging

# Define user agent
user_agent = "praw_scraper_1.0"

# Create an instance of reddit class
reddit = praw.Reddit(username="Deep_Swan8101",
                     password="12345678910",
                     client_id="q8QtPuIEfnX9XuYArJbvLw",
                     client_secret="m8AM5QZecEfCDgmegboF4qf0iCijqw",
                     user_agent=user_agent
)


# Create sub-reddit instance
subreddit_name = "lakers"
subreddit = reddit.subreddit(subreddit_name)

df = pd.DataFrame() # creating dataframe for displaying scraped data

# creating lists for storing scraped data
titles=[]
scores=[]
ids=[]

# looping over posts and scraping it
for submission in subreddit.new(limit=1000):
    titles.append(submission.title)
    scores.append(submission.score) #upvotes

logging.getLogger('praw').setLevel(logging.ERROR)  # Ignore warnings from PRAW

df['Title'] = titles
df['Upvotes'] = scores #upvotes

print(df)

positive_count = 0
negative_count = 0

# Analyze sentiment of each post
for post in titles:
    analysis = TextBlob(post)
    # Consider a post with a polarity > 0 as positive and < 0 as negative
    if analysis.sentiment.polarity > 0:
        positive_count += 1
    elif analysis.sentiment.polarity < 0:
        negative_count += 1
    # Neutral posts aren't counted towards the overall sentiment

# Determine overall sentiment
if positive_count > negative_count:
    overall_sentiment = 'positive'
elif negative_count > positive_count:
    overall_sentiment = 'negative'
else:
    overall_sentiment = 'neutral'  # or mixed if you have equal positive and negative

print(f"Overall sentiment: {overall_sentiment}. Positive posts: {positive_count}, Negative posts: {negative_count}.")

from googleapiclient.discovery import build
import os
!pip install pytube
from pytube import YouTube

youtube_api_key = 'AIzaSyATeMDWfP25d3yPBhTa8ne6TAKbvzToa8Y'

# Initialize the YouTube API client
youtube = build('youtube', 'v3', developerKey=youtube_api_key)
video_title= None

def search_youtube(query, max_results=1):
  global video_title
  request = youtube.search().list(
      part="snippet",
      maxResults=max_results,
      q=query,
      type="video"
    )
  response = request.execute()
  for search_result in response.get('items', []):
    video_id = search_result['id']['videoId']
    video_title = search_result['snippet']['title']
    video_url = "https://www.youtube.com/watch?v="+video_id
    Download(video_url)


def Download(link):
    youtubeObject = YouTube(link)
    youtubeObject = youtubeObject.streams.get_highest_resolution()
    try:
        youtubeObject.download()
    except:
        print("An error has occurred")
    print("Download is completed successfully")


video_urls = search_youtube("hooper highlights")

video_filename = "C:/Users/12012/Downloads/" + video_title + ".mp4"

!pip install Pillow

from moviepy.editor import VideoFileClip
from PIL import Image
import numpy as np
import math

# Load the video
clip = VideoFileClip("lac.mp4")

# Define the frame rate for extraction (e.g., 1 frame per 20 seconds)
frame_rate = 0.05  # This corresponds to 1 frame every 20 seconds

# Total number of frames to extract
total_frames = int(clip.duration * frame_rate)

# Extract and save frames
for i in range(total_frames):
    # Calculate the time for each frame
    t = i / frame_rate
    # Extract the frame
    frame = clip.get_frame(t)
    # Convert to an image
    frame_img = Image.fromarray(frame)
    # Save the image
    frame_img.save(f"frame_{i}.png")

# Load the saved images
images = [Image.open(f"frame_{i}.png") for i in range(total_frames)]

# Determine the number of rows and columns for the grid
num_images = len(images)
grid_size = math.ceil(math.sqrt(num_images))

# Ensure we have enough cells in the grid to fit all images
num_columns = grid_size
num_rows = math.ceil(num_images / num_columns)

# Assuming all images are the same size, get the width and height of the first image
img_width, img_height = images[0].size

# Create a new image with a size big enough to contain the grid
grid_width = img_width * num_columns
grid_height = img_height * num_rows
new_im = Image.new('RGB', (grid_width, grid_height))

# Paste images into the new image
for i, im in enumerate(images):
    x = i % num_columns * img_width
    y = i // num_columns * img_height
    new_im.paste(im, (x, y))

# Save the new image
new_im.save('grid.jpg')

!pip install openai-whisper

audio = clip.audio
audio.write_audiofile("audio.wav")

# Transcribe the audio using Whisper (needs OpenAI's Whisper model and API)
import whisper

model = whisper.load_model("base")
result = model.transcribe("audio.wav")
transcription = result["text"]
print(transcription)